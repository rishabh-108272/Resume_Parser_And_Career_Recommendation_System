{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a4e5847974a46158f32b8381f2c090e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_826cd65c501b44578a08c89ab6956799",
              "IPY_MODEL_9dad7ad55fba4b37932529eef6d0ab7c",
              "IPY_MODEL_3c8d3a9775024429802f0d697a30f267"
            ],
            "layout": "IPY_MODEL_6c3298ddbdf048658cb2d25078f5f40f"
          }
        },
        "826cd65c501b44578a08c89ab6956799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_248d001d13ec4e88b6cfafd6aaaeef3d",
            "placeholder": "​",
            "style": "IPY_MODEL_c2c6d0a0f62a424795431dd5ffcfdaea",
            "value": "Batches: 100%"
          }
        },
        "9dad7ad55fba4b37932529eef6d0ab7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea5d0eaf284344e28e706905b1eaa2b8",
            "max": 67,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9165e5bfe8dd42098d4f6b3ee2c47c2d",
            "value": 67
          }
        },
        "3c8d3a9775024429802f0d697a30f267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2df329c0c76c40c0917cf5190117a7e8",
            "placeholder": "​",
            "style": "IPY_MODEL_d07983fe18cd4c0187c2fde715d74e69",
            "value": " 67/67 [00:04&lt;00:00, 42.28it/s]"
          }
        },
        "6c3298ddbdf048658cb2d25078f5f40f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "248d001d13ec4e88b6cfafd6aaaeef3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c6d0a0f62a424795431dd5ffcfdaea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea5d0eaf284344e28e706905b1eaa2b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9165e5bfe8dd42098d4f6b3ee2c47c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2df329c0c76c40c0917cf5190117a7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07983fe18cd4c0187c2fde715d74e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlOsP8jrf0IS",
        "outputId": "55958476-2a89-433a-8b6b-5ceca4f68dd0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0a4e5847974a46158f32b8381f2c090e",
            "826cd65c501b44578a08c89ab6956799",
            "9dad7ad55fba4b37932529eef6d0ab7c",
            "3c8d3a9775024429802f0d697a30f267",
            "6c3298ddbdf048658cb2d25078f5f40f",
            "248d001d13ec4e88b6cfafd6aaaeef3d",
            "c2c6d0a0f62a424795431dd5ffcfdaea",
            "ea5d0eaf284344e28e706905b1eaa2b8",
            "9165e5bfe8dd42098d4f6b3ee2c47c2d",
            "2df329c0c76c40c0917cf5190117a7e8",
            "d07983fe18cd4c0187c2fde715d74e69"
          ]
        },
        "id": "daPS4cim_yJj",
        "outputId": "d86a54d6-1012-4e0a-d144-f3e728fd3a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not initialize Ollama client for local explanations: Connection error.\n",
            "This feature will be disabled. Recommendations will be direct QA results (and Gemini fallback).\n",
            "Gemini API configured successfully for fallback.\n",
            "New datasets (job_skills.csv, coursea_data.csv) loaded successfully for inference contexts.\n",
            "SentenceTransformer (embedding) model loaded successfully for RAG.\n",
            "Building contexts for FAISS index...\n",
            "Encoding 2141 contexts for FAISS...\n",
            "Using device for embedding: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/67 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a4e5847974a46158f32b8381f2c090e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built.\n",
            "Fine-tuned QA model loaded successfully.\n",
            "\n",
            "Starting Job and Course Recommendation System (Fine-tuned QA + RAG + Optional LLM Summary + Gemini Fallback).\n",
            "Type 'quit' at any prompt to exit.\n",
            "\n",
            "Enter your query (e.g., 'Google Cloud Program Manager responsibilities', 'courses on data science', 'Django developer jobs'): Recommend me Django Developer Jobs\n",
            "Which type of recommendation? (jobs/courses/all): all\n",
            "\n",
            "Searching for: 'Recommend me Django Developer Jobs'\n",
            "\n",
            "--- Detailed QA-Based Recommendations (Local Model) ---\n",
            "\n",
            "1. Type: Job, QA Score: 0.9999\n",
            "  Job Title: Quantitative Analyst, Ads Quality\n",
            "  Company: Google\n",
            "  Category: Product & Customer Support\n",
            "  Location: Zürich, Switzerland\n",
            "  Extracted Answer: Quantitative Analyst, Ads Quality\n",
            "  Responsibilities (Preview): Apply advanced statistical methods and work with large, complex data sets.\n",
            "Solve difficult, non-routine challenges, and clearly communicate highly tec...\n",
            "  Min Qualifications (Preview): PhD in Statistics or Econometrics or a related field, or equivalent practical experience.\n",
            "Experience in the analysis and modeling of data.\n",
            "Experience ...\n",
            "  Pref Qualifications (Preview): Relevant industry or research experience.\n",
            "Familiarity with both classical and Bayesian inference.\n",
            "Familiarity with experimental design principles.\n",
            "Abi...\n",
            "--------------------\n",
            "\n",
            "2. Type: Job, QA Score: 0.9999\n",
            "  Job Title: Data Science Analyst, Revenue Strategy and Operations, Google Marketing Solutions\n",
            "  Company: Google\n",
            "  Category: Sales Operations\n",
            "  Location: Mountain View, CA, United States\n",
            "  Extracted Answer: Data Science Analyst, Revenue Strategy and Operations, Google Marketing Solutions\n",
            "  Responsibilities (Preview): Build, productionize, launch and maintain predictive models\n",
            "Answer business questions through data analysis, statistical modeling and data mining.\n",
            "Wor...\n",
            "  Min Qualifications (Preview): Bachelor's degree in Computer Science, Statistics, Mathematics or equivalent practical experience.\n",
            "2 years of experience in statistical modeling, data...\n",
            "  Pref Qualifications (Preview): Master's degree in Statistics, Computer Science, Mathematics.\n",
            "Experience with languages R and SQL.\n",
            "Impeccable business judgment, with distinctive prob...\n",
            "--------------------\n",
            "\n",
            "3. Type: Job, QA Score: 0.9998\n",
            "  Job Title: Software Engineer, Accessibility\n",
            "  Company: Google\n",
            "  Category: Software Engineering\n",
            "  Location: Mountain View, CA, United States\n",
            "  Extracted Answer: Software Engineer, Accessibility\n",
            "  Responsibilities (Preview): Design, develop, test, deploy, maintain and improve software.\n",
            "Write solid, maintainable, well tested client code.\n",
            "Manage individual project priorities...\n",
            "  Min Qualifications (Preview): BA/BS degree in computer science, related technical field or equivalent practical experience.\n",
            "Programming experience in one or more of the following l...\n",
            "  Pref Qualifications (Preview): Master's or PhD in Computer Science or related technical field.\n",
            "1-year of relevant work experience, including experience with UNIX/Linux or Windows en...\n",
            "--------------------\n",
            "\n",
            "4. Type: Job, QA Score: 0.9997\n",
            "  Job Title: Front End Software Engineer\n",
            "  Company: Google\n",
            "  Category: Software Engineering\n",
            "  Location: Pittsburgh, PA, United States\n",
            "  Extracted Answer: Front End Software Engineer\n",
            "  Responsibilities (Preview): Build next-generation web applications with a focus on the client side.\n",
            "Redesign UI's, Implement new UI's, and pick up Java as necessary.\n",
            "Engage with ...\n",
            "  Min Qualifications (Preview): BA/BS degree or equivalent practical experience.\n",
            "1 year of work experience in software development.\n",
            "Experience with server-side web frameworks such as...\n",
            "  Pref Qualifications (Preview): 4 years of relevant work experience, including web application experience or skills using AJAX, HTML, CSS or JavaScript.\n",
            "Programming experience in GWT...\n",
            "--------------------\n",
            "\n",
            "5. Type: Job, QA Score: 0.9995\n",
            "  Job Title: UX Engineer, Front End, Waze\n",
            "  Company: Google\n",
            "  Category: User Experience & Design\n",
            "  Location: Tel Aviv-Yafo, Israel\n",
            "  Extracted Answer: UX Engineer, Front End, Waze\n",
            "  Responsibilities (Preview): Work closely with UX Designers, Project Managers and back-end Engineers to implement versatile front-end solutions to web development issues.\n",
            "Embrace ...\n",
            "  Min Qualifications (Preview): BA/BS degree in Computer Science, related technical field or equivalent practical experience.\n",
            "Experience with web technologies (e.g., object-oriented ...\n",
            "  Pref Qualifications (Preview): Experience in software development; in-depth experience developing numerous web-based applications.\n",
            "Experience with modern JavaScript frameworks (such...\n",
            "--------------------\n",
            "\n",
            "Are you satisfied with these recommendations? (yes/no/quit/gemini): gemini\n",
            "\n",
            "Initiating fallback to Gemini for recommendations...\n",
            "\n",
            "--- Generating recommendations from Gemini (Fallback) ---\n",
            "\n",
            "--- Recommendations from Gemini (Fallback) ---\n",
            "Okay, let's refine the Django developer job recommendations and provide a clearer path.  Previous automated responses often lack personalized context. Here's a targeted approach, assuming you have at least some existing Python knowledge.\n",
            "\n",
            "**I. Job Recommendations - Based on Experience Level (Pick the one that best fits you):**\n",
            "\n",
            "*   **Entry-Level (0-2 years experience):**\n",
            "    *   **Job Titles:** Django Developer (Junior), Python Developer (with Django), Software Engineer (Entry-Level, Django focus).\n",
            "    *   **Companies to Target:** Small to medium-sized startups.  Look at companies that are new, growing quickly, and have open-source projects that are built on Django.\n",
            "    *   **Why:** Startups often provide faster learning curves and more responsibility early on.  They're also often more willing to take a chance on someone showing potential.\n",
            "    *   **Example:** Check out listings on LinkedIn, Indeed, or smaller job boards like AngelList.  Specifically search for: \"Python Django Developer Entry Level\", filter for location.\n",
            "\n",
            "*   **Mid-Level (2-5 years experience):**\n",
            "    *   **Job Titles:** Django Developer, Python Developer, Software Engineer (Django Focus), Senior Backend Developer (Django).\n",
            "    *   **Companies to Target:**  Established tech companies, companies in industries like e-commerce, education, or media.\n",
            "    *   **Why:**  These companies likely have larger Django projects and more structured teams, offering mentorship and growth opportunities.  They can also offer more competitive salaries and benefits.\n",
            "    *   **Example:** Target Companies using Django. Examples are Mozilla, Instagram, Pinterest, and Disqus.\n",
            "\n",
            "*   **Senior-Level (5+ years experience):**\n",
            "    *   **Job Titles:** Senior Django Developer, Lead Django Developer, Software Architect (Django), Engineering Manager (Backend, Django).\n",
            "    *   **Companies to Target:** Larger tech companies, consulting firms specializing in Python/Django, or companies developing complex web applications.\n",
            "    *   **Why:** These roles demand expertise in architecture, design patterns, performance optimization, and team leadership.  The pay is higher, and the impact on the company is significant.\n",
            "    *   **Example:** Look at companies focused on building large scale infrastructure in Django. Focus on companies that use Django REST framework and containerize applications.\n",
            "\n",
            "**II. Course & Learning Path Recommendations (Focus on Specialization):**\n",
            "\n",
            "*   **Core Django Skill Strengthening:**\n",
            "    *   **Course:** \"The Complete Django Developer Course\" (various platforms like Udemy, Skillshare) - Aims to cover the whole ecosystem of Django.\n",
            "    *   **Why:**  Ensures a strong foundation in Django basics, project structure, ORM, and templating.\n",
            "    *   **Action:**  If you're already somewhat familiar with Django, focus on the sections covering advanced topics like Class-Based Views (CBVs), middleware, and custom template tags.\n",
            "\n",
            "*   **REST API Development (Highly Desirable Skill):**\n",
            "    *   **Course/Framework:**  Learn Django REST Framework (DRF).  Check out the official DRF documentation and tutorials.  Alternatively, consider a course like \"Django REST Framework: Build Powerful APIs with Python\" (Udemy).\n",
            "    *   **Why:**  Modern web applications heavily rely on APIs.  DRF is the industry-standard for building RESTful APIs with Django.\n",
            "    *   **Action:**  Build a small API project (e.g., a simple to-do list API) to solidify your understanding.\n",
            "\n",
            "*   **Testing & Deployment:**\n",
            "    *   **Topics:**  Learn how to write unit tests, integration tests, and end-to-end tests for Django applications.  Familiarize yourself with deployment using tools like Docker, Kubernetes, and cloud platforms (AWS, Google Cloud, Azure).\n",
            "    *   **Why:**  Testing ensures code quality, and deployment skills are essential for getting your applications live.\n",
            "    *   **Action:**  Integrate testing into your personal projects.  Deploy a small Django application to a cloud platform like Heroku.  The official Django documentation has guides on deployment and testing.\n",
            "\n",
            "*   **Frontend Development (Even Basic Knowledge Helps):**\n",
            "    *   **Topics:**  Learn the basics of HTML, CSS, and JavaScript.  Consider a framework like React, Vue.js, or Angular to build interactive frontends.\n",
            "    *   **Why:**  Having frontend skills allows you to build full-stack applications and better understand how your backend interacts with the frontend.\n",
            "    *   **Action:** Start a small project. Hookup Django API to React frontend.\n",
            "\n",
            "**III. Career Path Suggestions:**\n",
            "\n",
            "*   **Backend Developer:**  Focus on server-side logic, database management, and API development using Django.  This is the most common path for Django developers.\n",
            "*   **Full-Stack Developer:**  Develop both the frontend and backend of web applications using Django and a frontend framework.  Requires strong knowledge of both areas.\n",
            "*   **DevOps Engineer (with Django experience):**  Combine your Django knowledge with DevOps skills like cloud computing, automation, and CI/CD pipelines.\n",
            "*   **Technical Lead/Architect:**  Design and oversee the development of complex Django applications.  Requires extensive experience and leadership skills.\n",
            "\n",
            "**Key Takeaways & Encouragement:**\n",
            "\n",
            "*   **Focus on Building a Portfolio:**  Create personal projects that showcase your Django skills.  Contribute to open-source Django projects.  A strong portfolio is crucial for landing a job.\n",
            "*   **Network with Other Django Developers:** Attend meetups, conferences, and online forums.  Networking can help you learn about job opportunities and gain valuable insights.\n",
            "*   **Be Persistent:** The job market can be competitive, but don't give up!  Keep learning, improving your skills, and applying for jobs.  Tailor your resume and cover letter to each job you apply for.\n",
            "\n",
            "This approach is more actionable because it breaks down recommendations by experience level, suggests specific courses and frameworks to learn, and provides clear career path options.  It also emphasizes the importance of building a portfolio and networking with other developers. Good luck!\n",
            "\n",
            "\n",
            "Was Gemini's response helpful? (yes/no/quit): yes\n",
            "Feedback logged successfully.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Enter your query (e.g., 'Google Cloud Program Manager responsibilities', 'courses on data science', 'Django developer jobs'): quit\n",
            "Exiting recommendation system. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import datetime\n",
        "from openai import OpenAI # Used for Ollama local client (optional)\n",
        "import google.generativeai as genai # Import Google Generative AI library\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path where your fine-tuned model is saved from Part 2\n",
        "MODEL_PATH = \"/content/drive/MyDrive/fine_tuned_qa_model_job_course\" # Ensure this matches your fine-tuning script's output\n",
        "# Path to your original dataset CSV files\n",
        "DATASET_PATH = \"/content/drive/MyDrive/dataset/\" # Adjust this path\n",
        "# File to store feedback logs\n",
        "FEEDBACK_LOG_PATH = \"feedback_log_qa_job_course_v3_gemini.jsonl\" # New log file for this version\n",
        "\n",
        "# Ollama Client setup (Optional, for advanced LLM explanations/summaries from local LLM)\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "OLLAMA_MODEL = \"llama3\" # Or the model you pulled\n",
        "OLLAMA_TIMEOUT = 120.0\n",
        "\n",
        "llm_client = None # Initialize as None for Ollama\n",
        "try:\n",
        "    llm_client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\", timeout=OLLAMA_TIMEOUT)\n",
        "    llm_client.models.list() # Test connection\n",
        "    print(f\"Ollama client initialized for local model '{OLLAMA_MODEL}'. (Optional LLM for explanations)\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not initialize Ollama client for local explanations: {e}\")\n",
        "    print(\"This feature will be disabled. Recommendations will be direct QA results (and Gemini fallback).\")\n",
        "    llm_client = None\n",
        "\n",
        "\n",
        "# Gemini API setup (Fallback LLM)\n",
        "# *** CORRECT AND SECURE WAY TO GET API KEY ***\n",
        "GOOGLE_API_KEY = \"AIzaSyB8rM9gxGzUCOAfDDI-UBlRPcjGzgFBTY8\"\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"WARNING: GOOGLE_API_KEY environment variable not set. Gemini fallback will be unavailable.\")\n",
        "    print(\"Please set it (e.g., `export GOOGLE_API_KEY='YOUR_API_KEY'`) before running the script.\")\n",
        "    gemini_model = None\n",
        "else:\n",
        "    try:\n",
        "        genai.configure(api_key=GOOGLE_API_KEY) # The API key is used here from environment variable\n",
        "        # Test Gemini connection\n",
        "        gemini_test_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "        gemini_test_model.generate_content(\"hello\").text\n",
        "        gemini_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "        print(\"Gemini API configured successfully for fallback.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini API: {e}\")\n",
        "        print(\"Gemini fallback will be unavailable. Check your API key and network connection.\")\n",
        "        gemini_model = None\n",
        "\n",
        "\n",
        "# --- Load original datasets for contexts ---\n",
        "try:\n",
        "    jobs_df = pd.read_csv(os.path.join(DATASET_PATH, \"job_skills.csv\"))\n",
        "    courses_df = pd.read_csv(os.path.join(DATASET_PATH, \"coursea_data.csv\"))\n",
        "    print(\"New datasets (job_skills.csv, coursea_data.csv) loaded successfully for inference contexts.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: One or more dataset files not found. Please check the DATASET_PATH and filenames. {e}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading datasets: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Initialize Local Embedding Model for RAG Retrieval ---\n",
        "try:\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"SentenceTransformer (embedding) model loaded successfully for RAG.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading SentenceTransformer model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Prepare all contexts and build FAISS index for retrieval ---\n",
        "all_contexts_for_faiss = []\n",
        "all_context_metadata = []\n",
        "\n",
        "def create_context_entry_for_faiss(row, context_type):\n",
        "    if context_type == \"job\":\n",
        "        company = str(row.get('Company', '')).strip()\n",
        "        title = str(row.get('Title', '')).strip()\n",
        "        category = str(row.get('Category', '')).strip()\n",
        "        location = str(row.get('Location', '')).strip()\n",
        "        responsibilities = str(row.get('Responsibilities', '')).strip()\n",
        "        min_qual = str(row.get('Minimum Qualifications', '')).strip()\n",
        "        pref_qual = str(row.get('Preferred Qualifications', '')).strip()\n",
        "\n",
        "        text_parts = []\n",
        "        if title: text_parts.append(f\"Job Title: {title}\")\n",
        "        if company: text_parts.append(f\"Company: {company}\")\n",
        "        if category: text_parts.append(f\"Category: {category}\")\n",
        "        if location: text_parts.append(f\"Location: {location}\")\n",
        "        if responsibilities: text_parts.append(f\"Responsibilities: {responsibilities}\")\n",
        "        if min_qual: text_parts.append(f\"Minimum Qualifications: {min_qual}\")\n",
        "        if pref_qual: text_parts.append(f\"Preferred Qualifications: {pref_qual}\")\n",
        "\n",
        "        text = \". \".join(filter(None, text_parts))\n",
        "    elif context_type == \"course\":\n",
        "        course_title = str(row.get('course_title', '')).strip()\n",
        "        course_organization = str(row.get('course_organization', '')).strip()\n",
        "        course_certificate_type = str(row.get('course_Certificate_type', '')).strip()\n",
        "        course_rating = str(row.get('course_rating', '')).strip()\n",
        "        course_difficulty = str(row.get('course_difficulty', '')).strip()\n",
        "        course_students_enrolled = str(row.get('course_students_enrolled', '')).strip()\n",
        "\n",
        "        text_parts = []\n",
        "        if course_title: text_parts.append(f\"Course Title: {course_title}\")\n",
        "        if course_organization: text_parts.append(f\"Organization: {course_organization}\")\n",
        "        if course_certificate_type: text_parts.append(f\"Certificate Type: {course_certificate_type}\")\n",
        "        if course_rating: text_parts.append(f\"Rating: {course_rating}\")\n",
        "        if course_difficulty: text_parts.append(f\"Difficulty: {course_difficulty}\")\n",
        "        if course_students_enrolled: text_parts.append(f\"Students Enrolled: {course_students_enrolled}\")\n",
        "\n",
        "        text = \". \".join(filter(None, text_parts))\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "    if text.strip():\n",
        "        return text, {\"type\": context_type, \"original_data\": row.to_dict()}\n",
        "    return None, None\n",
        "\n",
        "# Populate contexts and build FAISS index\n",
        "print(\"Building contexts for FAISS index...\")\n",
        "for idx, row in jobs_df.iterrows():\n",
        "    context_text, metadata = create_context_entry_for_faiss(row, \"job\")\n",
        "    if context_text:\n",
        "        all_contexts_for_faiss.append(context_text)\n",
        "        all_context_metadata.append(metadata)\n",
        "\n",
        "for idx, row in courses_df.iterrows():\n",
        "    context_text, metadata = create_context_entry_for_faiss(row, \"course\")\n",
        "    if context_text:\n",
        "        all_contexts_for_faiss.append(context_text)\n",
        "        all_context_metadata.append(metadata)\n",
        "\n",
        "if not all_contexts_for_faiss:\n",
        "    print(\"No contexts generated for FAISS. Please check data processing.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Encoding {len(all_contexts_for_faiss)} contexts for FAISS...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device for embedding: {device}\")\n",
        "\n",
        "context_embeddings = embedding_model.encode(\n",
        "    all_contexts_for_faiss,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    device=device\n",
        ").astype('float32')\n",
        "\n",
        "dimension = context_embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "faiss_index.add(context_embeddings)\n",
        "print(\"FAISS index built.\")\n",
        "\n",
        "# --- Load the fine-tuned QA model and tokenizer ---\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
        "    print(\"Fine-tuned QA model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading fine-tuned QA model: {e}\")\n",
        "    print(\"Please ensure you have run the fine-tuning script (Part 2) and saved the model correctly.\")\n",
        "    exit()\n",
        "\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "# --- Feedback Logging Function ---\n",
        "def log_feedback(query, retrieved_qa_results, llm_response, user_feedback_text, source=\"Local_QA_RAG\"):\n",
        "    feedback_entry = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"query\": query,\n",
        "        \"source\": source, # Indicate if from Local QA or Gemini fallback\n",
        "        \"qa_results_preview\": [\n",
        "            {\n",
        "                \"score\": res.get('score'),\n",
        "                \"answer\": res.get('answer'),\n",
        "                \"context_type\": res.get('context_type'),\n",
        "                \"original_data_id\": res['original_data'].get('Unnamed: 0') or \\\n",
        "                                     res['original_data'].get('Title') if res['context_type'] == 'job' else \\\n",
        "                                     res['original_data'].get('course_title')\n",
        "            } for res in retrieved_qa_results\n",
        "        ],\n",
        "        \"llm_response_preview\": llm_response[:500] + \"...\" if len(llm_response) > 500 else llm_response,\n",
        "        \"user_feedback\": user_feedback_text,\n",
        "    }\n",
        "    with open(FEEDBACK_LOG_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(json.dumps(feedback_entry) + '\\n')\n",
        "    print(\"Feedback logged successfully.\")\n",
        "\n",
        "\n",
        "# --- Function to get recommendations from Gemini (Fallback) ---\n",
        "def get_gemini_recommendations(query, item_type=None):\n",
        "    # The API key is configured globally via genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    # So, we just need to use the gemini_model object directly.\n",
        "    if not gemini_model: # Check if the global gemini_model was successfully initialized\n",
        "        return \"Gemini API is not available or not configured.\"\n",
        "\n",
        "    print(\"\\n--- Generating recommendations from Gemini (Fallback) ---\")\n",
        "\n",
        "    # Construct a comprehensive prompt for Gemini\n",
        "    gemini_prompt = (\n",
        "        f\"The user is asking for recommendations based on the query: '{query}'.\\n\"\n",
        "        f\"They were not fully satisfied with previous automated recommendations. \"\n",
        "        f\"Please provide relevant and helpful recommendations for {item_type if item_type else 'jobs and courses'}. \"\n",
        "        f\"Suggest specific job titles, companies, course names, and organizations where applicable. \"\n",
        "        f\"Explain why these are good recommendations and suggest a potential career/learning path.\"\n",
        "        f\"Be concise, actionable, and encouraging.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = gemini_model.generate_content(gemini_prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error communicating with Gemini API: {e}\"\n",
        "\n",
        "# --- Recommendation Function with RAG and Optional LLM Summary ---\n",
        "def get_qa_recommendations_with_rag(query: str, k: int = 5, item_type: str = None, num_retrieved_contexts: int = 10):\n",
        "    print(f\"\\nSearching for: '{query}'\")\n",
        "\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True).astype('float32')\n",
        "    D, I = faiss_index.search(query_embedding, num_retrieved_contexts)\n",
        "\n",
        "    retrieved_contexts_for_qa = []\n",
        "    system_info_for_log = []\n",
        "\n",
        "    for idx in I[0]:\n",
        "        if idx >= 0 and idx < len(all_contexts_for_faiss):\n",
        "            item_metadata = all_context_metadata[idx]\n",
        "            if item_type and item_metadata['type'] != item_type:\n",
        "                continue\n",
        "\n",
        "            context_text = all_contexts_for_faiss[idx]\n",
        "\n",
        "            try:\n",
        "                qa_result = qa_pipeline(question=query, context=context_text)\n",
        "\n",
        "                if qa_result['score'] > 0.05 and qa_result['answer'].strip() != \"\":\n",
        "                    retrieved_contexts_for_qa.append({\n",
        "                        \"score\": qa_result['score'],\n",
        "                        \"answer\": qa_result['answer'],\n",
        "                        \"context_type\": item_metadata[\"type\"],\n",
        "                        \"original_data\": item_metadata[\"original_data\"],\n",
        "                        \"full_context_text\": context_text\n",
        "                    })\n",
        "                    system_info_for_log.append({\n",
        "                        \"retrieved_context_content_preview\": context_text[:200] + \"...\",\n",
        "                        \"qa_answer\": qa_result['answer'],\n",
        "                        \"qa_score\": qa_result['score']\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    retrieved_contexts_for_qa.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    final_qa_recommendations = []\n",
        "    seen_unique_ids = set()\n",
        "    for res in retrieved_contexts_for_qa:\n",
        "        unique_id = None\n",
        "        if res['context_type'] == 'job':\n",
        "            unique_id = res['original_data'].get('Unnamed: 0')\n",
        "            if unique_id is None: unique_id = res['original_data'].get('Title') + \"_\" + res['original_data'].get('Company')\n",
        "        elif res['context_type'] == 'course':\n",
        "            unique_id = res['original_data'].get('Unnamed: 0')\n",
        "            if unique_id is None: unique_id = res['original_data'].get('course_title')\n",
        "\n",
        "        if unique_id is not None and unique_id not in seen_unique_ids:\n",
        "            final_qa_recommendations.append(res)\n",
        "            seen_unique_ids.add(unique_id)\n",
        "            if len(final_qa_recommendations) >= k:\n",
        "                break\n",
        "\n",
        "    llm_summary_text = \"\"\n",
        "    if llm_client and final_qa_recommendations: # Only generate local LLM summary if local client is active AND we have recommendations\n",
        "        llm_prompt_context = \"\"\n",
        "        for i, rec in enumerate(final_qa_recommendations):\n",
        "            llm_prompt_context += f\"Recommendation {i+1} (Type: {rec['context_type'].upper()}):\\n\"\n",
        "            if rec['context_type'] == 'job':\n",
        "                llm_prompt_context += f\"  Job Title: {rec['original_data'].get('Title')}\\n\"\n",
        "                llm_prompt_context += f\"  Company: {rec['original_data'].get('Company')}\\n\"\n",
        "                llm_prompt_context += f\"  Key Info Extracted: {rec['answer']}\\n\"\n",
        "                llm_prompt_context += f\"  Responsibilities Preview: {rec['original_data'].get('Responsibilities', '')[:100]}...\\n\"\n",
        "            elif rec['context_type'] == 'course':\n",
        "                llm_prompt_context += f\"  Course Title: {rec['original_data'].get('course_title')}\\n\"\n",
        "                llm_prompt_context += f\"  Organization: {rec['original_data'].get('course_organization')}\\n\"\n",
        "                llm_prompt_context += f\"  Key Info Extracted: {rec['answer']}\\n\"\n",
        "                llm_prompt_context += f\"  Difficulty: {rec['original_data'].get('course_difficulty')}\\n\"\n",
        "            llm_prompt_context += \"---\\n\"\n",
        "\n",
        "        system_message = (\n",
        "            \"You are an AI assistant that provides career and learning recommendations. \"\n",
        "            \"You have retrieved specific job and course details based on a user's query. \"\n",
        "            \"Your task is to synthesize these details into a concise, personalized summary. \"\n",
        "            \"Explain *why* these specific items are relevant to the user's query and suggest a potential next step or learning path.\"\n",
        "            \"Do NOT just re-list the items. Focus on explanation and actionable advice.\"\n",
        "        )\n",
        "        user_message = (\n",
        "            f\"The user's query is: '{query}'\\n\\n\"\n",
        "            f\"Here are {len(final_qa_recommendations)} specific recommendations with extracted key information:\\n\\n\"\n",
        "            f\"{llm_prompt_context}\\n\"\n",
        "            f\"Please provide a personalized summary and a suggested path.\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = llm_client.chat.completions.create(\n",
        "                model=OLLAMA_MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                temperature=0.7,\n",
        "                max_tokens=600\n",
        "            )\n",
        "            llm_summary_text = response.choices[0].message.content\n",
        "            print(\"\\n--- Local LLM's Personalized Recommendation & Summary ---\")\n",
        "            print(llm_summary_text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling local Ollama LLM for summary: {e}\")\n",
        "            llm_summary_text = \"Could not generate a local LLM summary.\"\n",
        "\n",
        "    return final_qa_recommendations, llm_summary_text\n",
        "\n",
        "# --- Main Interaction Loop ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nStarting Job and Course Recommendation System (Fine-tuned QA + RAG + Optional LLM Summary + Gemini Fallback).\")\n",
        "    print(\"Type 'quit' at any prompt to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nEnter your query (e.g., 'Google Cloud Program Manager responsibilities', 'courses on data science', 'Django developer jobs'): \").strip()\n",
        "        if user_query.lower() == 'quit':\n",
        "            print(\"Exiting recommendation system. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        rec_type_input = input(\"Which type of recommendation? (jobs/courses/all): \").strip().lower()\n",
        "        if rec_type_input not in ['jobs', 'courses', 'all']:\n",
        "            print(\"Invalid type. Please choose 'jobs', 'courses', or 'all'.\")\n",
        "            continue\n",
        "\n",
        "        item_type_filter = rec_type_input if rec_type_input != 'all' else None\n",
        "\n",
        "        # --- STEP 1: Get recommendations from local QA + RAG model ---\n",
        "        recommendations, llm_summary = get_qa_recommendations_with_rag(\n",
        "            user_query,\n",
        "            k=5,\n",
        "            item_type=item_type_filter,\n",
        "            num_retrieved_contexts=10\n",
        "        )\n",
        "\n",
        "        # --- Display local recommendations ---\n",
        "        if not recommendations:\n",
        "            print(f\"No {rec_type_input} recommendations found with sufficient QA confidence from local model.\")\n",
        "        else:\n",
        "            print(\"\\n--- Detailed QA-Based Recommendations (Local Model) ---\")\n",
        "            for i, rec in enumerate(recommendations):\n",
        "                print(f\"\\n{i+1}. Type: {rec['context_type'].capitalize()}, QA Score: {rec['score']:.4f}\")\n",
        "                if rec['context_type'] == 'job':\n",
        "                    print(f\"  Job Title: {rec['original_data'].get('Title')}\")\n",
        "                    print(f\"  Company: {rec['original_data'].get('Company')}\")\n",
        "                    print(f\"  Category: {rec['original_data'].get('Category')}\")\n",
        "                    print(f\"  Location: {rec['original_data'].get('Location')}\")\n",
        "                    print(f\"  Extracted Answer: {rec['answer']}\")\n",
        "                    print(f\"  Responsibilities (Preview): {rec['original_data'].get('Responsibilities', '')[:150]}...\")\n",
        "                    print(f\"  Min Qualifications (Preview): {rec['original_data'].get('Minimum Qualifications', '')[:150]}...\")\n",
        "                    print(f\"  Pref Qualifications (Preview): {rec['original_data'].get('Preferred Qualifications', '')[:150]}...\")\n",
        "                elif rec['context_type'] == 'course':\n",
        "                    print(f\"  Course Title: {rec['original_data'].get('course_title')}\")\n",
        "                    print(f\"  Organization: {rec['original_data'].get('course_organization')}\")\n",
        "                    print(f\"  Certificate Type: {rec['original_data'].get('course_Certificate_type')}\")\n",
        "                    print(f\"  Rating: {rec['original_data'].get('course_rating')}\")\n",
        "                    print(f\"  Difficulty: {rec['original_data'].get('course_difficulty')}\")\n",
        "                    print(f\"  Students Enrolled: {rec['original_data'].get('course_students_enrolled')}\")\n",
        "                    print(f\"  Extracted Answer: {rec['answer']}\")\n",
        "                print(\"-\" * 20)\n",
        "\n",
        "        # --- STEP 2: Handle User Satisfaction and Gemini Fallback ---\n",
        "        user_satisfaction = input(\"\\nAre you satisfied with these recommendations? (yes/no/quit/gemini): \").strip().lower()\n",
        "\n",
        "        if user_satisfaction == 'quit':\n",
        "            log_feedback(user_query, recommendations, llm_summary, \"Quit_Session_After_Local_Attempt\", source=\"Local_QA_RAG\")\n",
        "            print(\"Exiting recommendation system. Goodbye!\")\n",
        "            break\n",
        "        elif user_satisfaction == 'yes':\n",
        "            log_feedback(user_query, recommendations, llm_summary, \"Satisfied\", source=\"Local_QA_RAG\")\n",
        "            print(\"Great! Glad I could help.\")\n",
        "        elif user_satisfaction == 'no' or user_satisfaction == 'gemini' or not recommendations:\n",
        "            # If user is not satisfied OR no local recommendations were found\n",
        "            if gemini_model:\n",
        "                print(\"\\nInitiating fallback to Gemini for recommendations...\")\n",
        "                gemini_output = get_gemini_recommendations(user_query, item_type=item_type_filter)\n",
        "                print(\"\\n--- Recommendations from Gemini (Fallback) ---\")\n",
        "                print(gemini_output)\n",
        "\n",
        "                final_feedback_after_gemini = input(\"\\nWas Gemini's response helpful? (yes/no/quit): \").strip().lower()\n",
        "                log_feedback(user_query, [], gemini_output, final_feedback_after_gemini, source=\"Gemini_Fallback\") # Log Gemini response\n",
        "                if final_feedback_after_gemini == 'quit':\n",
        "                    print(\"Exiting recommendation system. Goodbye!\")\n",
        "                    break\n",
        "            else:\n",
        "                print(\"\\nGemini fallback not available. Please configure your GOOGLE_API_KEY.\")\n",
        "                log_feedback(user_query, recommendations, llm_summary, \"Not_Satisfied_No_Gemini_Fallback\", source=\"Local_QA_RAG\")\n",
        "        else:\n",
        "            # Catch any other specific feedback if 'no' or 'gemini' wasn't explicit\n",
        "            log_feedback(user_query, recommendations, llm_summary, user_satisfaction, source=\"Local_QA_RAG\")\n",
        "            print(\"Thanks for your feedback!\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\") # Separator for next interaction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_yGb3iVFO81",
        "outputId": "790cc27c-81e8-4359-f1d1-10b7b040f26d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "IrAN4TF4FRSe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P1ZbNNOuF2IF"
      }
    }
  ]
}